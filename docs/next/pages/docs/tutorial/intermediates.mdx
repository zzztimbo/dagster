<h1 id="tutorial-intermediates">Intermediates</h1>
<div class="toctree" data-maxdepth="1" hidden="">

</div>
<p>We've already seen how solids can describe their persistent artifacts to the system using <a href="materializations.html">materializations</a>.</p>
<p>Dagster also has a facility for automatically materializing the intermediate values that actually pass between solids.</p>
<p>This can be very useful for debugging, when you want to inspect the value output by a solid and ensure that it is as you expect; for audit, when you want to understand how a particular downstream output was created; and for re-executing downstream solids with cached results from expensive upstream computations.</p>
<p>To turn intermediate storage on, just set another key in the pipeline config:</p>
<div class="literalinclude" data-linenos="" data-emphasize-lines="6-7" data-caption="intermediates.yaml" data-language="YAML">
<p>../../../examples/dagster_examples/intro_tutorial/intermediates.yaml</p>
</div>
<p>When you execute the pipeline using this config, you'll see new structured entries in the Dagit log viewer indicating that intermediates have been stored on the filesystem.</p>
<div class="thumbnail">
<p>intermediates.png</p>
</div>
<h2 id="intermediate-storage-for-types-that-cannot-be-pickled">Intermediate storage for types that cannot be pickled</h2>
<p>By default, Dagster will try to pickle intermediate values to store them on the filesystem. Some custom data types cannot be pickled (for instance, a Spark RDD), so you will need to tell Dagster how to serialize them.</p>
<p>Our toy <code>LessSimpleDataFrame</code> is, of course, pickleable, but supposing it was not, let's set a custom :py<code class="interpreted-text" role="class">SerializationStrategy &lt;dagster.SerializationStrategy&gt;</code> on it to tell Dagster how to store intermediates of this type.</p>
<div class="literalinclude" data-lines="16-39" data-linenos="" data-lineno-start="16" data-caption="serialization_strategy.py" data-language="python">
<p>../../../examples/dagster_examples/intro_tutorial/serialization_strategy.py</p>
</div>
<p>Now, when we set the <code>storage</code> key in pipeline config and run this pipeline, we'll see that our intermediate is automatically persisted as a human-readable .csv:</p>
<div class="thumbnail">
<p>serialization_strategy.png</p>
</div>
<h2 id="reexecution">Reexecution</h2>
<p>Once intermediates are being stored, Dagit makes it possible to individually execute solids whose outputs are satisfied by previously materialized intermediates.</p>
<p>Click on the <code>sort_by_calories.compute</code> execution step, and you'll see the option appear to reexecute only this step, using the automatically materialized intermediate output of the previous solid.</p>
<div class="thumbnail">
<p>reexecution.png</p>
</div>
<div class="thumbnail">
<p>reexecution_results.png</p>
</div>
<p>Reexecuting individual solids can be very helpful while you're writing solids, or while you're actively debugging them.</p>
<p>You can also manually specify intermediates from previous runs as inputs to solids. Recall the syntax we used to set input values using the config system:</p>
<div class="literalinclude" data-language="YAML" data-linenos="" data-caption="inputs_env.yaml">
<p>../../../examples/dagster_examples/intro_tutorial/inputs_env.yaml</p>
</div>
<p>Instead of setting the key <code>value</code> (i.e., providing a ), we can also set <code>pickle</code>, as follows:</p>
<div class="literalinclude" data-language="YAML" data-linenos="" data-caption="reexecution_env.yaml">
<p>../../../examples/dagster_examples/intro_tutorial/reexecution_env.yaml</p>
</div>
<p>(Of course, you'll need to use the path to an intermediate that is actually present on your filesystem.)</p>
<p>If you directly substitute this config into Dagit, you'll see an error, because the system still expects the input to <code>sort_by_calories</code> to be satisfied by the output from <code>read_csv</code>.</p>
<div class="thumbnail">
<p>reexecution_errors.png</p>
</div>
<p>To make this config valid, we'll need to tell Dagit to execute only a subset of the pipeline --just the <code>sort_by_calories</code> solid. Click on the subset-selector button in the top left of the playground, to the left of the Mode selector (which, when no subset has been specified, will read "*"):</p>
<div class="thumbnail">
<p>subset_selection.png</p>
</div>
<p>Hit "Apply", and this config will now pass validation, and the individual solid can be reexecuted:</p>
<div class="thumbnail">
<p>subset_config.png</p>
</div>
<p>This facility is especially valuable during test, since it allows you to validate newly written solids against values generated during previous runs of a known good pipeline.</p>
